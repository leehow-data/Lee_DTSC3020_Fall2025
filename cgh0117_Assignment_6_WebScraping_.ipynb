{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehow-data/Lee_DTSC3020_Fall2025/blob/main/cgh0117_Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858784e0-e1c6-4460-ea89-d1e2810b51f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "#1) Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0ad8ee-72db-4373-9fe7-71ce5af28a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    tables = pd.read_html(html, header=0)\n",
        "\n",
        "    # Pick the first table with at least 3 columns\n",
        "    df = next((t for t in tables if t.shape[1] >= 3), None)\n",
        "    if df is None:\n",
        "        raise ValueError(\"No table with ≥3 columns found in the given HTML.\")\n",
        "\n",
        "    # Flatten any MultiIndex columns into single level strings\n",
        "    df.columns = [' '.join(map(str, col)).strip() if isinstance(col, tuple) else str(col).strip()\n",
        "                  for col in df.columns]\n",
        "\n",
        "    return df\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\"\"\"\n",
        "    # Standardize column names (trim spaces, title case)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # Rename common variations just in case (robust to source differences)\n",
        "    rename_map = {\n",
        "        'Country': 'Country',\n",
        "        'Country Name': 'Country',\n",
        "        'Alpha-2 code': 'Alpha-2',\n",
        "        'Alpha2': 'Alpha-2',\n",
        "        'Alpha-3 code': 'Alpha-3',\n",
        "        'Alpha3': 'Alpha-3',\n",
        "        'Numeric code': 'Numeric',\n",
        "        'Numeric': 'Numeric'\n",
        "    }\n",
        "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
        "\n",
        "    # Keep only expected columns (ignore extras)\n",
        "    expected = ['Country', 'Alpha-2', 'Alpha-3', 'Numeric']\n",
        "    df = df[[c for c in expected if c in df.columns]]\n",
        "\n",
        "    # Strip whitespace\n",
        "    for col in ['Country', 'Alpha-2', 'Alpha-3']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    # Uppercase Alpha-2 and Alpha-3\n",
        "    for col in ['Alpha-2', 'Alpha-3']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].str.upper()\n",
        "\n",
        "    # Convert Numeric to nullable int (e.g., \"004\" → 4)\n",
        "    if 'Numeric' in df.columns:\n",
        "        df['Numeric'] = (\n",
        "            pd.to_numeric(df['Numeric'], errors='coerce')\n",
        "            .astype('Int64')  # nullable integer dtype\n",
        "        )\n",
        "\n",
        "    # Drop rows missing critical data\n",
        "    df = df.dropna(subset=['Country', 'Alpha-2', 'Alpha-3'])\n",
        "\n",
        "    return df\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    if 'Numeric' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'Numeric' column.\")\n",
        "\n",
        "    # Sort descending by Numeric, drop rows with missing values in Numeric\n",
        "    df_sorted = df.dropna(subset=['Numeric']).sort_values(\n",
        "        by='Numeric', ascending=False\n",
        "    )\n",
        "\n",
        "    # Return top-N rows\n",
        "    return df_sorted.head(top).reset_index(drop=True)\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b20d274-69ec-4881-e366-599eafa60e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Country Alpha-2 Alpha-3  Numeric\n",
            "0                                              Zambia      ZM     ZMB      894\n",
            "1                                               Yemen      YE     YEM      887\n",
            "2                                               Samoa      WS     WSM      882\n",
            "3                                   Wallis and Futuna      WF     WLF      876\n",
            "4                  Venezuela (Bolivarian Republic of)      VE     VEN      862\n",
            "5                                          Uzbekistan      UZ     UZB      860\n",
            "6                                             Uruguay      UY     URY      858\n",
            "7                                        Burkina Faso      BF     BFA      854\n",
            "8                               Virgin Islands (U.S.)      VI     VIR      850\n",
            "9                      United States of America (the)      US     USA      840\n",
            "10                       Tanzania, United Republic of      TZ     TZA      834\n",
            "11                                        Isle of Man      IM     IMN      833\n",
            "12                                             Jersey      JE     JEY      832\n",
            "13                                           Guernsey      GG     GGY      831\n",
            "14  United Kingdom of Great Britain and Northern I...      GB     GBR      826\n",
            "\n",
            "✅ Saved cleaned data to /content/data_q1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-737383951.py:6: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html, header=0)\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your answer here\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://www.iban.com/country-codes\"\n",
        "html = requests.get(url).text\n",
        "\n",
        "# 1. Read\n",
        "df_raw = q1_read_table(html)\n",
        "\n",
        "# 2. Clean\n",
        "df_clean = q1_clean(df_raw)\n",
        "\n",
        "# 3. Sort and preview top 15\n",
        "top15 = q1_sort_top(df_clean, top=15)\n",
        "print(top15)\n",
        "\n",
        "# 4. Save to CSV\n",
        "csv_path = Path(\"data_q1.csv\")\n",
        "df_clean.to_csv(csv_path, index=False)\n",
        "print(f\"\\n✅ Saved cleaned data to {csv_path.resolve()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 – Country Codes Extraction and Cleaning\n",
        "\n",
        "**Source:** [https://www.iban.com/country-codes](https://www.iban.com/country-codes)\n",
        "\n",
        "### Steps\n",
        "1. Fetch the HTML page using `requests`.\n",
        "2. Use `pandas.read_html()` to read all tables and select the first with ≥3 columns.\n",
        "3. Clean the data:\n",
        "   - Trim spaces from all text columns.\n",
        "   - Convert `Alpha-2` and `Alpha-3` to uppercase.\n",
        "   - Convert `Numeric` codes to integers (nullable, removing leading zeros).\n",
        "4. Save the cleaned dataset as **`data_q1.csv`**.\n",
        "5. Sort descending by `Numeric` and print the **Top 15** rows (no charts).\n",
        "\n",
        "### Output Files\n",
        "- `data_q1.csv` — Cleaned dataset\n",
        "- `q1_country_codes.ipynb` — Notebook with all steps\n",
        "- Console output — Top-15 countries sorted by `Numeric` (descending)\n",
        "\n",
        "### Limitation\n",
        "The dataset comes from a public third-party source (IBAN.com) which may not always reflect the most up-to-date ISO 3166-1 country codes. Some territories or special regions may differ from the official ISO list.\n"
      ],
      "metadata": {
        "id": "98fCfansqppx"
      },
      "id": "98fCfansqppx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    items = []\n",
        "\n",
        "    # Each story row is a .athing element\n",
        "    for story in soup.select(\".athing\"):\n",
        "        rank_tag = story.select_one(\".rank\")\n",
        "        title_tag = story.select_one(\".titleline a\")\n",
        "\n",
        "        rank = rank_tag.text.strip().replace(\".\", \"\") if rank_tag else \"0\"\n",
        "        title = title_tag.text.strip() if title_tag else \"\"\n",
        "        link = title_tag[\"href\"].strip() if title_tag and title_tag.has_attr(\"href\") else \"\"\n",
        "\n",
        "        # The next <tr> with class 'subtext' has details\n",
        "        subtext = story.find_next_sibling(\"tr\").select_one(\".subtext\")\n",
        "\n",
        "        points_tag = subtext.select_one(\".score\") if subtext else None\n",
        "        user_tag = subtext.select_one(\".hnuser\") if subtext else None\n",
        "        comment_tag = subtext.find_all(\"a\")[-1] if subtext and subtext.find_all(\"a\") else None\n",
        "\n",
        "        points = points_tag.text if points_tag else \"0 points\"\n",
        "        user = user_tag.text if user_tag else \"\"\n",
        "        comments_text = comment_tag.text if comment_tag else \"0 comments\"\n",
        "\n",
        "        items.append({\n",
        "            \"rank\": rank,\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"points\": points,\n",
        "            \"comments\": comments_text,\n",
        "            \"user\": user\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    Cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Fill missing text fields\n",
        "    text_cols = [\"title\", \"link\", \"user\"]\n",
        "    for col in text_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
        "\n",
        "    # Extract digits from numeric-like fields\n",
        "    for col in [\"points\", \"comments\", \"rank\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col].astype(str)\n",
        "                .str.extract(r\"(\\d+)\")\n",
        "                .fillna(0)\n",
        "                .astype(int)\n",
        "            )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    if \"points\" not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'points' column.\")\n",
        "    df_sorted = df.sort_values(\"points\", ascending=False).reset_index(drop=True)\n",
        "    return df_sorted.head(top)\n",
        "\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q2_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b6f83b-2bd3-477e-c46e-f32e6fb51f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    rank                                              title  \\\n",
            "0     20                    Why Nextcloud feels slow to use   \n",
            "1      5             Ask HN: Who is hiring? (November 2025)   \n",
            "2     14                          The Case Against PGVector   \n",
            "3     26                        </> Htmx – The Fetch()ening   \n",
            "4      7  Learning to read Arthur Whitney's C to become ...   \n",
            "5     15  A visualization of the RGB space covered by na...   \n",
            "6     17  WebAssembly (WASM) arch support for the Linux ...   \n",
            "7     16  First recording of a dying human brain shows w...   \n",
            "8     13  State of Terminal Emulators in 2025: The Erran...   \n",
            "9      2                                   AI's Dial-Up Era   \n",
            "10    21                                           VimGraph   \n",
            "11     9     Ask HN: Who wants to be hired? (November 2025)   \n",
            "12     8  The Mack Super Pumper was a locomotive engined...   \n",
            "13    19  Skyfall-GS – Synthesizing Immersive 3D Urban S...   \n",
            "14    30  Why engineers can't be rational about programm...   \n",
            "\n",
            "                                                 link  points  comments  \\\n",
            "0   https://ounapuu.ee/posts/2025/11/03/nextcloud-...     374       290   \n",
            "1                                    item?id=45800465     303       328   \n",
            "2   https://alex-jacobs.com/posts/the-case-against...     274       106   \n",
            "3             https://htmx.org/essays/the-fetchening/     255        89   \n",
            "4   https://needleful.net/blog/2024/01/arthur_whit...     238        88   \n",
            "5              https://codepen.io/meodai/full/zdgXJj/     228        52   \n",
            "6           https://github.com/joelseverin/linux-wasm     220        52   \n",
            "7   https://louisville.edu/medicine/news/first-eve...     204       190   \n",
            "8   https://www.jeffquast.com/post/state-of-termin...     158       135   \n",
            "9        https://www.wreflection.com/p/ai-dial-up-era     156       138   \n",
            "10  https://resources.wolframcloud.com/FunctionRep...     143        26   \n",
            "11                                   item?id=45800464     133       253   \n",
            "12  https://bangshift.com/bangshiftxl/mack-super-p...     111        73   \n",
            "13                    https://skyfall-gs.jayinnn.dev/     106        28   \n",
            "14       https://spf13.com/p/the-hidden-conversation/      98       114   \n",
            "\n",
            "              user  \n",
            "0            rpgbr  \n",
            "1      whoishiring  \n",
            "2      tacoooooooo  \n",
            "3      leephillips  \n",
            "4          gudzpoz  \n",
            "5      BlankCanvas  \n",
            "6       marcodiego  \n",
            "7      thunderbong  \n",
            "8              SG-  \n",
            "9          nowflux  \n",
            "10      gdelfino01  \n",
            "11     whoishiring  \n",
            "12          mstngl  \n",
            "13  ChrisArchitect  \n",
            "14           spf13  \n",
            "\n",
            "✅ Saved to data_q2.csv\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your answer here\n",
        "\n",
        "import requests\n",
        "\n",
        "# Fetch HTML\n",
        "url = \"https://news.ycombinator.com/\"\n",
        "html = requests.get(url).text\n",
        "\n",
        "# Parse, clean, and sort\n",
        "df_raw = q2_parse_items(html)\n",
        "df_clean = q2_clean(df_raw)\n",
        "top15 = q2_sort_top(df_clean, top=15)\n",
        "\n",
        "# Save and display\n",
        "df_clean.to_csv(\"data_q2.csv\", index=False)\n",
        "print(top15)\n",
        "print(\"\\n✅ Saved to data_q2.csv\")\n",
        "\n"
      ],
      "id": "q2_skeleton_answer"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}